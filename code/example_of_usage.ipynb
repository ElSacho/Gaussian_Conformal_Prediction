{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data and declare hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from network import *\n",
    "\n",
    "from data_loading import *\n",
    "\n",
    "from gaussian_predictor_levelsets import *\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (32010, 7) Y_train shape: (32010, 3)\n",
      "X_cal shape: (4573, 7) Y_cal shape: (4573, 3)\n",
      "X_test shape: (4574, 7) Y_test shape: (4574, 3)\n",
      "X_stop shape: (4573, 7) Y_stop shape: (4573, 3)\n"
     ]
    }
   ],
   "source": [
    "load_path = \"../data/processed_data_3Dmin/casp.npz\"\n",
    "\n",
    "X, Y = load_data(load_path)\n",
    "\n",
    "normalize = True\n",
    "splits = [0.7, 0.1, 0.1, 0.1]\n",
    "\n",
    "subsets = split_and_preprocess(X, Y, splits=splits, normalize=normalize)\n",
    "\n",
    "x_train, y_train, x_calibration, y_calibration, x_test, y_test, x_stop, y_stop = subsets[\"X_train\"], subsets[\"Y_train\"], subsets[\"X_calibration\"], subsets[\"Y_calibration\"], subsets[\"X_test\"], subsets[\"Y_test\"], subsets[\"X_stop\"], subsets[\"Y_stop\"]\n",
    "\n",
    "print(\"X_train shape:\", x_train.shape, \"Y_train shape:\", y_train.shape)\n",
    "print(\"X_cal shape:\", x_calibration.shape, \"Y_cal shape:\", y_calibration.shape)\n",
    "print(\"X_test shape:\", x_test.shape, \"Y_test shape:\", y_test.shape)\n",
    "print(\"X_stop shape:\", x_stop.shape, \"Y_stop shape:\", y_stop.shape)\n",
    "\n",
    "\n",
    "d = x_train.shape[1]\n",
    "k = y_train.shape[1]\n",
    "\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "n_calibration = x_calibration.shape[0]\n",
    "n_stop = x_stop.shape[0]\n",
    "\n",
    "hidden_dim = 256\n",
    "hidden_dim_matrix = 256\n",
    "n_hidden_layers = 3\n",
    "n_hidden_layers_matrix = 1\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "lr_center = 1e-3\n",
    "lr_matrix = 1e-3\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "use_lr_scheduler = True\n",
    "keep_best = True\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=dtype)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=dtype)\n",
    "x_stop_tensor = torch.tensor(x_stop, dtype=dtype)\n",
    "y_stop_tensor = torch.tensor(y_stop, dtype=dtype)\n",
    "x_calibration_tensor = torch.tensor(x_calibration, dtype=dtype)\n",
    "y_calibration_tensor = torch.tensor(y_calibration, dtype=dtype)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=dtype)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=dtype)\n",
    "\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.29114311933517456, Validation Loss: 0.3018602728843689\n",
      "Epoch: 2, Train Loss: 0.2846681475639343, Validation Loss: 0.29662442207336426\n",
      "Epoch: 3, Train Loss: 0.26599064469337463, Validation Loss: 0.277357280254364\n",
      "Epoch: 4, Train Loss: 0.25823506712913513, Validation Loss: 0.2687288820743561\n",
      "Epoch: 5, Train Loss: 0.24677784740924835, Validation Loss: 0.25828099250793457\n",
      "Epoch: 6, Train Loss: 0.24114175140857697, Validation Loss: 0.2523420453071594\n",
      "Epoch: 7, Train Loss: 0.23863165080547333, Validation Loss: 0.2508649528026581\n",
      "Epoch: 8, Train Loss: 0.22714680433273315, Validation Loss: 0.2393798828125\n",
      "Epoch: 9, Train Loss: 0.21856781840324402, Validation Loss: 0.22969989478588104\n",
      "Epoch: 10, Train Loss: 0.21495552361011505, Validation Loss: 0.22901980578899384\n",
      "Epoch: 11, Train Loss: 0.20943695306777954, Validation Loss: 0.2228257805109024\n",
      "Epoch: 12, Train Loss: 0.20912602543830872, Validation Loss: 0.2241537868976593\n",
      "Epoch: 13, Train Loss: 0.2013183981180191, Validation Loss: 0.21544621884822845\n",
      "Epoch: 14, Train Loss: 0.20141597092151642, Validation Loss: 0.22026850283145905\n",
      "Epoch: 15, Train Loss: 0.19240699708461761, Validation Loss: 0.21172311902046204\n",
      "Epoch: 16, Train Loss: 0.19028769433498383, Validation Loss: 0.20863518118858337\n",
      "Epoch: 17, Train Loss: 0.19760248064994812, Validation Loss: 0.21931961178779602\n",
      "Epoch: 18, Train Loss: 0.18516306579113007, Validation Loss: 0.2066538780927658\n",
      "Epoch: 19, Train Loss: 0.1814139038324356, Validation Loss: 0.20378842949867249\n",
      "Epoch: 20, Train Loss: 0.17817842960357666, Validation Loss: 0.20241302251815796\n",
      "Epoch: 21, Train Loss: 0.17917239665985107, Validation Loss: 0.1994371861219406\n",
      "Epoch: 22, Train Loss: 0.17134904861450195, Validation Loss: 0.19701285660266876\n",
      "Epoch: 23, Train Loss: 0.17148961126804352, Validation Loss: 0.1956307739019394\n",
      "Epoch: 24, Train Loss: 0.16557353734970093, Validation Loss: 0.18976455926895142\n",
      "Epoch: 25, Train Loss: 0.1647159904241562, Validation Loss: 0.19039386510849\n",
      "Epoch: 26, Train Loss: 0.1653156876564026, Validation Loss: 0.1913873702287674\n",
      "Epoch: 27, Train Loss: 0.15934909880161285, Validation Loss: 0.1873551607131958\n",
      "Epoch: 28, Train Loss: 0.1581135094165802, Validation Loss: 0.188550665974617\n",
      "Epoch: 29, Train Loss: 0.15484967827796936, Validation Loss: 0.18429593741893768\n",
      "Epoch: 30, Train Loss: 0.15182296931743622, Validation Loss: 0.18315130472183228\n",
      "Epoch: 31, Train Loss: 0.14912042021751404, Validation Loss: 0.1822095811367035\n",
      "Epoch: 32, Train Loss: 0.15370427072048187, Validation Loss: 0.19118048250675201\n",
      "Epoch: 33, Train Loss: 0.14629435539245605, Validation Loss: 0.18196313083171844\n",
      "Epoch: 34, Train Loss: 0.1457311511039734, Validation Loss: 0.1845691055059433\n",
      "Epoch: 35, Train Loss: 0.14787006378173828, Validation Loss: 0.1885208934545517\n",
      "Epoch: 36, Train Loss: 0.14343813061714172, Validation Loss: 0.18546593189239502\n",
      "Epoch: 37, Train Loss: 0.14287614822387695, Validation Loss: 0.18347394466400146\n",
      "Epoch: 38, Train Loss: 0.14075443148612976, Validation Loss: 0.18363656103610992\n",
      "Epoch: 39, Train Loss: 0.13706530630588531, Validation Loss: 0.17990635335445404\n",
      "Epoch: 40, Train Loss: 0.13304241001605988, Validation Loss: 0.18074151873588562\n",
      "Epoch: 41, Train Loss: 0.1338014453649521, Validation Loss: 0.1795312464237213\n",
      "Epoch: 42, Train Loss: 0.1306755244731903, Validation Loss: 0.1767268180847168\n",
      "Epoch: 43, Train Loss: 0.1305486559867859, Validation Loss: 0.18276426196098328\n",
      "Epoch: 44, Train Loss: 0.12946155667304993, Validation Loss: 0.1790882796049118\n",
      "Epoch: 45, Train Loss: 0.12791557610034943, Validation Loss: 0.17885378003120422\n",
      "Epoch: 46, Train Loss: 0.12665419280529022, Validation Loss: 0.18206533789634705\n",
      "Epoch: 47, Train Loss: 0.12533757090568542, Validation Loss: 0.18046478927135468\n",
      "Epoch: 48, Train Loss: 0.1275576651096344, Validation Loss: 0.1869901418685913\n",
      "Epoch: 49, Train Loss: 0.1201072558760643, Validation Loss: 0.17939873039722443\n",
      "Epoch: 50, Train Loss: 0.11782113462686539, Validation Loss: 0.1784452497959137\n",
      "Epoch: 51, Train Loss: 0.11939795315265656, Validation Loss: 0.17847128212451935\n",
      "Epoch: 52, Train Loss: 0.11839844286441803, Validation Loss: 0.18145430088043213\n",
      "Epoch: 53, Train Loss: 0.12017211318016052, Validation Loss: 0.18093888461589813\n",
      "Epoch: 54, Train Loss: 0.11222304403781891, Validation Loss: 0.17647944390773773\n",
      "Epoch: 55, Train Loss: 0.11326757073402405, Validation Loss: 0.1774560660123825\n",
      "Epoch: 56, Train Loss: 0.11198928207159042, Validation Loss: 0.18111567199230194\n",
      "Epoch: 57, Train Loss: 0.11067140102386475, Validation Loss: 0.17940738797187805\n",
      "Epoch: 58, Train Loss: 0.11739203333854675, Validation Loss: 0.1831221580505371\n",
      "Epoch: 59, Train Loss: 0.10951961576938629, Validation Loss: 0.17927420139312744\n",
      "Epoch: 60, Train Loss: 0.11377456784248352, Validation Loss: 0.18503819406032562\n",
      "Epoch: 61, Train Loss: 0.1090635359287262, Validation Loss: 0.17969636619091034\n",
      "Epoch: 62, Train Loss: 0.10580704361200333, Validation Loss: 0.18171314895153046\n",
      "Epoch: 63, Train Loss: 0.10389851778745651, Validation Loss: 0.18154919147491455\n",
      "Epoch: 64, Train Loss: 0.10407408326864243, Validation Loss: 0.1803724318742752\n",
      "Epoch: 65, Train Loss: 0.10076753050088882, Validation Loss: 0.18164321780204773\n",
      "Epoch: 66, Train Loss: 0.09955871105194092, Validation Loss: 0.17937931418418884\n",
      "Epoch: 67, Train Loss: 0.09953463822603226, Validation Loss: 0.1814773976802826\n",
      "Epoch: 68, Train Loss: 0.10091995447874069, Validation Loss: 0.18156571686267853\n",
      "Epoch: 69, Train Loss: 0.10087244212627411, Validation Loss: 0.18439702689647675\n",
      "Epoch: 70, Train Loss: 0.09371238201856613, Validation Loss: 0.17836551368236542\n",
      "Epoch: 71, Train Loss: 0.09492840617895126, Validation Loss: 0.18162302672863007\n",
      "Epoch: 72, Train Loss: 0.09560032188892365, Validation Loss: 0.17959478497505188\n",
      "Epoch: 73, Train Loss: 0.09358896315097809, Validation Loss: 0.18062394857406616\n",
      "Epoch: 74, Train Loss: 0.09142197668552399, Validation Loss: 0.18293964862823486\n",
      "Epoch: 75, Train Loss: 0.09510285407304764, Validation Loss: 0.18605802953243256\n",
      "Epoch: 76, Train Loss: 0.08923999220132828, Validation Loss: 0.18277613818645477\n",
      "Epoch: 77, Train Loss: 0.09045374393463135, Validation Loss: 0.1843833327293396\n",
      "Epoch: 78, Train Loss: 0.08968310803174973, Validation Loss: 0.1863250732421875\n",
      "Epoch: 79, Train Loss: 0.09084382653236389, Validation Loss: 0.1883421540260315\n",
      "Epoch: 80, Train Loss: 0.08587761968374252, Validation Loss: 0.1855158656835556\n",
      "Epoch: 81, Train Loss: 0.08450593054294586, Validation Loss: 0.18269343674182892\n",
      "Epoch: 82, Train Loss: 0.08813858777284622, Validation Loss: 0.18616236746311188\n",
      "Epoch: 83, Train Loss: 0.0879426971077919, Validation Loss: 0.18738462030887604\n",
      "Epoch: 84, Train Loss: 0.08489615470170975, Validation Loss: 0.18755033612251282\n",
      "Epoch: 85, Train Loss: 0.08204562962055206, Validation Loss: 0.18385055661201477\n",
      "Epoch: 86, Train Loss: 0.0848056972026825, Validation Loss: 0.18633688986301422\n",
      "Epoch: 87, Train Loss: 0.08265867084264755, Validation Loss: 0.1863040030002594\n",
      "Epoch: 88, Train Loss: 0.08452336490154266, Validation Loss: 0.18555355072021484\n",
      "Epoch: 89, Train Loss: 0.08258761465549469, Validation Loss: 0.19056515395641327\n",
      "Epoch: 90, Train Loss: 0.08241095393896103, Validation Loss: 0.19051611423492432\n",
      "Epoch: 91, Train Loss: 0.08351867645978928, Validation Loss: 0.18579775094985962\n",
      "Epoch: 92, Train Loss: 0.07998005300760269, Validation Loss: 0.19188663363456726\n",
      "Epoch: 93, Train Loss: 0.07597680389881134, Validation Loss: 0.18701182305812836\n",
      "Epoch: 94, Train Loss: 0.07567153126001358, Validation Loss: 0.18865734338760376\n",
      "Epoch: 95, Train Loss: 0.07728469371795654, Validation Loss: 0.19046354293823242\n",
      "Epoch: 96, Train Loss: 0.07514875382184982, Validation Loss: 0.1874420940876007\n",
      "Epoch: 97, Train Loss: 0.07830104231834412, Validation Loss: 0.19394385814666748\n",
      "Epoch: 98, Train Loss: 0.07681074738502502, Validation Loss: 0.1937732845544815\n",
      "Epoch: 99, Train Loss: 0.0747971311211586, Validation Loss: 0.19080375134944916\n",
      "Epoch: 100, Train Loss: 0.07446449249982834, Validation Loss: 0.19097894430160522\n",
      "Center model trained.\n"
     ]
    }
   ],
   "source": [
    "# Declare the architecture of your models and train the predictive model \n",
    "\n",
    "center_model = Network(d, k, hidden_dim=hidden_dim, n_hidden_layers=n_hidden_layers).to(dtype)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor), batch_size= batch_size, shuffle=True)\n",
    "stoploader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_stop_tensor, y_stop_tensor), batch_size= batch_size, shuffle=True)\n",
    "calibrationloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_calibration_tensor, y_calibration_tensor), batch_size= batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor), batch_size= batch_size, shuffle=True)\n",
    "\n",
    "center_model.fit_and_plot(trainloader, \n",
    "                            stoploader, \n",
    "                            num_epochs, \n",
    "                            keep_best = keep_best, \n",
    "                            lr = lr_center, \n",
    "                            verbose=True)\n",
    "\n",
    "print(\"Center model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = -0.4322127103805542 - Stop Loss = 0.37005889415740967 - Best Stop Loss = 0.37005889415740967\n",
      "Epoch 10: Loss = -2.139697790145874 - Stop Loss = -1.273223638534546 - Best Stop Loss = -1.341976284980774\n",
      "Epoch 20: Loss = -2.1895294189453125 - Stop Loss = -1.2386797666549683 - Best Stop Loss = -1.3813153505325317\n",
      "Epoch 30: Loss = -2.2191967964172363 - Stop Loss = -1.2883884906768799 - Best Stop Loss = -1.3813153505325317\n",
      "Epoch 40: Loss = -2.2183196544647217 - Stop Loss = -1.3062115907669067 - Best Stop Loss = -1.3813153505325317\n",
      "Epoch 50: Loss = -2.258356809616089 - Stop Loss = -1.2830098867416382 - Best Stop Loss = -1.3813153505325317\n",
      "Epoch 60: Loss = -2.2880120277404785 - Stop Loss = -1.2162033319473267 - Best Stop Loss = -1.3813153505325317\n",
      "Epoch 70: Loss = -2.2982425689697266 - Stop Loss = -1.2753283977508545 - Best Stop Loss = -1.3813153505325317\n",
      "Epoch 80: Loss = -2.309662342071533 - Stop Loss = -1.2629238367080688 - Best Stop Loss = -1.3813153505325317\n",
      "Epoch 90: Loss = -2.3127493858337402 - Stop Loss = -1.2679150104522705 - Best Stop Loss = -1.3813153505325317\n",
      "Final Loss = -2.15097713470459 - Final Stop Loss = -1.3843039274215698 - Best Stop Loss = -1.3813153505325317\n"
     ]
    }
   ],
   "source": [
    "# Train the covariance matrix on top of your predictive model\n",
    "\n",
    "matrix_model = MatrixPredictor(d, k, k, hidden_dim=hidden_dim_matrix, n_hidden_layers=0).to(dtype)\n",
    "\n",
    "gaussian_level_sets = GaussianPredictorLevelsets(center_model, matrix_model, dtype=dtype)\n",
    "\n",
    "gaussian_level_sets.fit(trainloader, \n",
    "                        stoploader, \n",
    "                        num_epochs = num_epochs,\n",
    "                        lr_center_models = 0.0,  # This value can change if you want to update the center model\n",
    "                        lr_matrix_models = lr_matrix,\n",
    "                        use_lr_scheduler = use_lr_scheduler,\n",
    "                        verbose = 1,\n",
    "                        stop_on_best = keep_best\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9996)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_level_sets.conformalize(x_calibration=x_calibration_tensor, y_calibration=y_calibration_tensor, alpha = alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.9147354613030171\n",
      "Average Volume: 1.5655988454818726\n"
     ]
    }
   ],
   "source": [
    "coverage = gaussian_level_sets.get_coverage(x_test_tensor, y_test_tensor)\n",
    "volumes  = gaussian_level_sets.get_averaged_volume(x_test_tensor).item()\n",
    "\n",
    "print(\"Coverage:\", coverage)\n",
    "print(\"Average Volume:\", volumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revealed outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3888)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_knowned = np.array([0])\n",
    "\n",
    "gaussian_level_sets.conformalize_with_knowned_idx(x_calibration=x_calibration_tensor, \n",
    "                                            y_calibration=y_calibration_tensor, \n",
    "                                            alpha = alpha, \n",
    "                                            idx_knowned=idx_knowned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.9140795802361172\n",
      "Average Volume: 1.5636283159255981\n"
     ]
    }
   ],
   "source": [
    "coverage = gaussian_level_sets.get_coverage_condition_on_idx(x_test_tensor, y_test_tensor)\n",
    "volumes  = gaussian_level_sets.get_averaged_volume_condition_on_idx(x_test_tensor, y_test_tensor[:, idx_knowned]).item()\n",
    "\n",
    "print(\"Coverage:\", coverage)\n",
    "print(\"Average Volume:\", volumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5912)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection_matrix_tensor =  torch.randn((2, k), dtype=dtype)\n",
    "\n",
    "gaussian_level_sets.conformalize_linear_projection(\n",
    "                                            projection_matrix=projection_matrix_tensor,\n",
    "                                            x_calibration=x_calibration_tensor, \n",
    "                                            y_calibration=y_calibration_tensor, \n",
    "                                            alpha = alpha\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.9138609532138172\n",
      "Average Volume: 2.412954092025757\n"
     ]
    }
   ],
   "source": [
    "coverage = gaussian_level_sets.get_coverage_projection(x_test_tensor, y_test_tensor)\n",
    "volumes  = gaussian_level_sets.get_averaged_volume_projection(x_test_tensor).item()\n",
    "\n",
    "print(\"Coverage:\", coverage)\n",
    "print(\"Average Volume:\", volumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add NaN values to the calibration and test sets\n",
    "\n",
    "y_calibration_nan = add_nan(y_calibration, min_nan=1, max_nan=k-1)\n",
    "y_calibration_nan_tensor = torch.tensor(y_calibration_nan, dtype=dtype)\n",
    "\n",
    "y_test_nan = add_nan(y_test, min_nan=1, max_nan=k-1)\n",
    "y_test_nan_tensor = torch.tensor(y_test_nan, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0172, dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gaussian_predictor_missing_outputs import *\n",
    "\n",
    "gaussian_predictor_missing_outputs = GaussianPredictorMissingValues(center_model, matrix_model, dtype=dtype)\n",
    "\n",
    "gaussian_predictor_missing_outputs.conformalize(x_calibration=x_calibration_tensor, y_calibration=y_calibration_nan_tensor, alpha = alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage with NaN: 0.9053344993441189\n",
      "Coverage full vector: 0.8388718845649322\n"
     ]
    }
   ],
   "source": [
    "coverage_with_nan    = gaussian_predictor_missing_outputs.get_coverage(x_test_tensor, y_test_nan_tensor)\n",
    "coverage_full_vector = gaussian_predictor_missing_outputs.get_coverage(x_test_tensor, y_test_tensor)\n",
    "\n",
    "print(\"Coverage with NaN:\", coverage_with_nan)\n",
    "print(\"Coverage full vector:\", coverage_full_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
